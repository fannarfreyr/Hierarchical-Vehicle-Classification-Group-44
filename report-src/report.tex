  \documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{float}



\title{A template for the \emph{arxiv} style}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\hypersetup{
pdftitle={Hierarchical Vehicle Classification with Multi-Task Learning},
pdfsubject={Computer Science, Deep Learning, Classification},
pdfauthor={Author One Name, Author Two Name, Author Three Name}, % Update these names
pdfkeywords={Fine-Grained Classification, Multi-Task Learning, ResNet, Hierarchical Label Smoothing},
}

% --- TITLE AND AUTHORS ---
\title{Hierarchical Vehicle Classification with Multi-Task Learning}

\author{
    Jónuson Fannar Freyr \thanks{Department of Computer Science, Aarhus University} \\
    \and 
    Jenrik Engels \footnotemark[1] \\
    \and
    András Szabolcs Gyüre \footnotemark[1]
}

\begin{document}
\maketitle

\begin{abstract}
	This project investigates effective strategies for hierarchical image classification on the Stanford Cars Dataset, addressing the challenge of distinguishing between 196 classes of car models. We used a ResNet-50 backbone and evaluate three distinct output designs: a flat single-head classifier, a two-head classifier, and a three-head Multi-Task Learning (MTL) architecture that independently predicts the vehicle's Make, Type, and Model. We demonstrate that the three-head design, specifically when optimized with Curriculum Learning (CL) and Hierarchical Label Smoothing (HLS), significantly outperforms simpler, flat classification methods. The inclusion of the "Type" prediction head proved critical, acting as a necessary semantic bridge to guide the feature extractor. This structured approach enforced strong taxonomic coherence, driving prediction consistency between Make and Model to $97.66\%$. Our final model, stabilized by Test Time Augmentation (TTA), achieved a Top-1 accuracy of $88.57\%$, confirming that for complex, fine-grained tasks, structuring the learning process is paramount to achieving robust and logically coherent predictions.
\end{abstract}


% keywords can be removed
\keywords{First keyword \and Second keyword \and More}


\section{Introduction}

Fine-grained vehicle classification, which differentiates between various types, makes, and models of cars, presents significant challenges. This difficulty arises from the large number of visually similar vehicle categories, variations in lighting and weather conditions, occlusions, and the diverse viewpoints from which vehicles may be captured. As a result, building a robust vehicle classification system requires a model capable of recognizing subtle visual cues while generalizing across highly variable real-world conditions.

In this project, we aim to explore and compare different strategies for hierarchical image classification on the Stanford Cars Dataset (Krause et al., 2013). The dataset contains 16,185 high-resolution images labeled with three hierarchical attributes: vehicle make, vehicle type, and vehicle model. These labels naturally form a multi-level taxonomy, making the dataset well-suited for studying hierarchical classification approaches.

Our baseline approach uses a ResNet-50 model pretrained on ImageNet as the backbone architecture. Initially, we treat the problem as a flat classification task by predicting the complete car label (containing make, type, and model) as a single class among 196 possible categories. This serves as our starting point for evaluating how well a standard single-head classifier performs on fine-grained classification without explicit hierarchical structure.

Building on this baseline, the primary objective of this project is to investigate how different hierarchical output designs influence classification performance. Specifically, we focus on three classification strategies:

\begin{enumerate}
    \item \textbf{Single-head (flat) classifier:}
    Predicts the entire label (make, type, and model) as one combined class. This ignores the hierarchical relationships.
    \item \textbf{Two-head classifier:}
    One head predicts the vehicle make, while the second head predicts the combined type+model label. 
    \item \textbf{Three-head classifier:}
    Predicts make, type, and model independently using three parallel classification heads.
\end{enumerate}   

After we evaluated these 3 variants, we will further enhance their performance using targeted regularization and optimization strategies. These include data augmentation, dropout, weight decay, and learning-rate adjustments, all of which can help reduce overfitting.

\section{Related Work}
Since the Stanford Cars dataset is popular, it is possible to find plenty prior work, trying to achieve high accuracy for classification by using different types of models and strategies. \newline
The paper “A Systematic Evaluation of Recent Deep Learning Architectures for Fine-Grained Vehicle Classification” by Valev et al. investigates how modern convolutional neural networks perform on the Stanford Cars dataset. The authors evaluate several well-known CNN architectures, including VGG16, Inception, MobileNet, and multiple residual networks, with an emphasis on ResNet-50, ResNet-152, and DenseNet variants. Their study focuses entirely on flat fine-grained classification, meaning that each car is classified directly into one of 196 labels. The method does not employ any form of hierarchical structure such as predicting make or type separately. ResNet-50 is tested both when trained from scratch and when fine-tuned from ImageNet weights. Training ResNet-50 from scratch yields an accuracy of 84.3\%, whereas fine-tuning dramatically improves performance to 92.0\%. The deeper ResNet-152 shows similar behavior and achieves only 35.3\% when trained from scratch but rises to 92.6\% when fine-tuned. Among all evaluated CNNs, the best performing model is DenseNet-161, which reaches 94.6\% accuracy on the Stanford Cars dataset Their experiments highlight the importance of data augmentation such as horizontal flipping and motion blur, which add up to meaningful improvements. The paper does not consider hierarchical labels or multi-head predictions that leverage the natural make-type-model structure of the dataset. (https://arxiv.org/pdf/1806.02987) \newline
The paper “Fine\-Grained Image Classification for Vehicle Makes \& Models using CNNs” explores the challenge of distinguishing highly similar car models within the Stanford Cars dataset. For their approach, they rely on transfer learning with a VGG-16 backbone pretrained on ImageNet. Two model variants are evaluated: a baseline that fine-tunes only the fully connected layers, and an improved architecture that removes one of the original fully connected layers, reduces the dimensionality of the remaining layers, and introduces dropout for regularization. The authors apply extensive data augmentation. Their method predicts all 196 classes as a single flat classification task. The experimental results show that the baseline VGG-16 achieves only 52.1\% accuracy on the test set, while the modified VGG-16 reaches a much stronger accuracy of 84.0. These findings demonstrate that fine-tuned CNN backbones, combined with aggressive augmentation and streamlined classification heads, can perform competitively on fine-grained vehicle recognition tasks. (https://cs230.stanford.edu/projects\_spring\_2019/reports/18681590.pdf) \newline
Our approach follows a similar foundation in that we also rely on transfer learning, starting from a ResNet-50 backbone pretrained on ImageNet. However, instead of only treating the Stanford Cars dataset as a flat 196-class problem, we additionally explore hierarchical classification strategies. Concretely, we design and compare models with one, two, and three prediction heads, where each head independently predicts one of the hierarchical levels: make, type, and model. Additionally, our approach incorporates a broader set of regularization and stabilization techniques, such as data augmentation, dropout, weight decay, and learning-rate scheduling. Finally, we also evaluate the impact of test-time augmentation (TTA). 

\section{Methods}
\subsection{Dataset and Pre-Processing??}

	We used the Stanford Cars Dataset, which comprises a total of 16,185 images distributed across 196 distinct classes of automobiles. The dataset was divided almost evenly for training and testing: 8,144 images were designated for the training set, and the remaining 8,041 images formed the test set. Importantly, the split for each of the 196 car classes was maintained at approximately a 50-50 ratio between the training and testing partitions.
	\newline Hierarchical Label Definiton: table comes here
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Hierarchy Level} &
\textbf{Description} &
\textbf{Example} &
\textbf{Classification Difficulty} \\
\hline

Head 1: Make &
\begin{tabular}[c]{@{}c@{}}The manufacturer or brand of the vehicle.\\ This is the coarse-grained class.\end{tabular} &
BMW &
\begin{tabular}[c]{@{}c@{}}Easy \\ (Highly distinct features/logos)\end{tabular} \\
\hline

Head 2: Type &
\begin{tabular}[c]{@{}c@{}}The general body style of the vehicle,\\ inferred from the model name.\end{tabular} &
Sedan &
\begin{tabular}[c]{@{}c@{}}Medium \\ (Defined by shape/proportions)\end{tabular} \\
\hline

Head 3: Model &
\begin{tabular}[c]{@{}c@{}}The specific model and year of\\ the vehicle (the original 196 classes).\end{tabular} &
3-Series Sedan 2012 &
\begin{tabular}[c]{@{}c@{}}Hard \\ (Subtle visual differences)\end{tabular} \\
\hline
\end{tabular}
\vspace{6pt}
\caption{Hierarchical Label Definition}
\end{table}


\subsection{Model Architecture}

    The core of the model is a ResNet-50 network, which serves as the backbone and was initialized using weights pre-trained on ImageNet without any subsequent architectural modifications. The overall Data Architecture is rooted in the Shared-Bottom Multi-Task Learning paradigm. This approach involves using the single Convolutional Neural Network (CNN) backbone to extract a universal, high-dimensional feature representation from the input image. This shared feature is then simultaneously fed into multiple, distinct "heads," where each head is responsible for its own classification task (e.g., Make, Model, or Type).
    \newline Specifically, the Multi-Head Architecture utilizes this paradigm by taking the shared feature vector, $F_{shared}$, extracted by the backbone, and splitting it to feed three independent, fully-connected (Dense) classification heads: Make, Type, and Model. This design enables the backbone to learn general, low-level visual characteristics common to all tasks, while each specific head refines the features needed for its respective hierarchical classification level. The entire network is trained end-to-end by optimizing a single Combined Training Objective, $L_{Total}$. This total loss function is defined as the weighted sum of the Categorical Cross-Entropy (CCE) losses ($L_{CCE}$) calculated from each head's prediction:
    $$L_{Total} = w_{Make} \cdot L_{CCE}(y_{Make}, \hat{y}_{Make}) + w_{Type} \cdot L_{CCE}(y_{Type}, \hat{y}_{Type}) + w_{Model} \cdot L_{CCE}(y_{Model}, \hat{y}_{Model})$$


\subsection{Training Strategies}
\subsubsection*{Baseline}
 First we set up a baseline for training which contained batch size = 32 with 15 epochs and a 0.0001 learning rate, where the ImageNet layers were frozen and only the fully connected layer was unfrozen. Then we unfroze the rest of the model parameters, to help the model recognize the shapes of the elements of the car models. Then we added the Make classification head that only predicts the make of the car (fx. BMW) while the first head was predicting the full label to make sure that the model is learning the hierarchy. Then later we added a third, Type classification head that only predicts the type of the car (fx. Sedan)
	
\subsubsection*{Data Augmentation, Normalization}
We added 4 different types of data augmentation and normalization.
        Random Resized: and Cropped Images were first resized to a larger   dimension (256x256) and then a $224 \times 224$ area was randomly     cropped. This combination forces the model to learn features that are   robust to variations in position, scale, and perspective (i.e., the car   is not always perfectly centered).
        Random Horizontal Flip: Flips the image along the vertical axis with a  50% probability, teaching the model to recognize features regardless of  their left-right orientation.
        Random Rotation: Images were randomly rotated up to $\pm 15$ degrees.   This provides rotational invariance, which is particularly useful for     handling slight tilts or oblique angles in real-world images.
        Color Jitter: Randomly alters the brightness, contrast, and saturation of   the images to make the model invariant to lighting conditions.
        Normalization: Input images were normalized using the mean and standard deviation of the ImageNet dataset, matching the pre-training conditions of the ResNet backbone.
 
\subsubsection*{Curriculum Learning}
 We addressed curriculum learning with the 3 model head to make the model first learn easy (Make) and medium (Type) tasks before focusing on the hard (Model) task. To achieve it, we increased the number of epochs to 20 and in the 5 epochs, we froze the Model head, so it only trains on the Make and Type heads.
	

\subsubsection*{Hierarchical Label Smoothing}
	While we were trying to squeeze out the last percentages of accuracy in training, we used HLS, to make the model penalize more the easy misses then the hard misses. Which means that for example mistaking a BMW M3 for a BMW 328i (Sibling error) shouln't give the same loss as mistaking a a BMW M3 for a Ford F-150 (Distant error). The technique we used for it is instead of a hard [0, 1, 0, 0] target, use soft targets that give partial credit to siblings. While the hard target gives 100\% loss for the correct car, while the soft targets give 90\% to the correct car and 10\% distributed among other cars in the same Make 

\begin{itemize}
	\item \textbf{Baseline:} First we set up a baseline for training which contained batch size = 32 with 15 epochs and a 0.0001 learning rate, where the ImageNet layers were frozen and only the fully connected layer was unfrozen. Then we unfroze the rest of the model parameters, to help the model recognize the shapes of the elements of the car models. Then we added the Make classification head that only predicts the make of the car (fx. BMW) while the first head was predicting the full label to make sure that the model is learning the hierarchy. Then later we added a third, Type classification head that only predicts the type of the car (fx. Sedan)
	\item \textbf{Data Augmentation, Normalization:} We added 4 different types of data augmentation and normalization
       \item Random Resized: and Cropped Images were first resized to a larger   dimension (256x256) and then a $224 \times 224$ area was randomly     cropped. This combination forces the model to learn features that are   robust to variations in position, scale, and perspective (i.e., the car   is not always perfectly centered).
    
        \item Random Horizontal Flip: Flips the image along the vertical axis with a  50% probability, teaching the model to recognize features regardless of  their left-right orientation.
    
        \item Random Rotation: Images were randomly rotated up to $\pm 15$ degrees.   This provides rotational invariance, which is particularly useful for     handling slight tilts or oblique angles in real-world images.
    
        \item Color Jitter: Randomly alters the brightness, contrast, and saturation of   the images to make the model invariant to lighting conditions.
    
        \item Normalization: Input images were normalized using the mean and standard deviation of the ImageNet dataset, matching the pre-training conditions of the ResNet backbone.
	\item \textbf{Curriculum Learning:} We addressed curriculum learning with the 3 model head to make the model first learn easy (Make) and medium (Type) tasks before focusing on the hard (Model) task. To achieve it, we increased the number of epochs to 20 and in the 5 epochs, we froze the Model head, so it only trains on the Make and Type heads.
	\item \textbf{Hierarchical Label Smoothing (HLS):} While we were trying to squeeze out the last percentages of accuracy in training, we used HLS, to make the model penalize more the easy misses then the hard misses. Which means that for example mistaking a BMW M3 for a BMW 328i (Sibling error) shouln't give the same loss as mistaking a a BMW M3 for a Ford F-150 (Distant error). The technique we used for it is instead of a hard [0, 1, 0, 0] target, use soft targets that give partial credit to siblings. While the hard target gives 100\% loss for the correct car, while the soft targets give 90\% to the correct car and 10\% distributed among other cars in the same Make 
\end{itemize}

\subsection{Inference Strategies:}
To evaluate the robustness of our model, we employ Test Time Augmentation (TTA) during the inference phase. Standard inference utilizes a single center crop of the input image. In contrast, our TTA protocol averages the softmax probability distributions of $N=2$ views: the original image ($x$) and a horizontally flipped version ($flip(x)$). The final prediction $\hat{y}$ is computed as:

$$\hat{y} = \text{argmax} \left( \frac{1}{2} (P(x) + P(flip(x))) \right)$$

This approach exploits the model's learned invariance to geometric transformations (due to the data augmentation in Section 5.4) to stabilize and often improve final predictions. We assumed that it is going to slightly increase our accuracy during testing.

\section{Results}
Organize results by "Research Question" rather than date.

\subsection{Establishing the "Hierarchy Gap" (Baselines)}

	The objective of this section is to show that flat classifiers fail to capture relationships. This was done by comparing the performance on the data using the Frozen Baseline (achieving 41.72\% Acc) against the Unfrozen model (achieving approximately 75.92\% Acc). The key finding demonstrated that even with a decent overall accuracy, the "Gap" between the Make Accuracy (85.49\%) and the Model Accuracy (75.92\%) was nearly 10\%. This indicates that the model was essentially guessing the Models without reliably knowing the underlying Brand.


\subsection{The Impact of Regularization \& Class Balancing}

	The objective here was to solve the overfitting problem. This was demonstrated using the data to show a significant jump from approximately 75.92\% accuracy (observed in Phase 3) to 85.65\% accuracy (achieved in Phase 4) simply by adding Data Augmentation. The key observation is that we gained nearly 10\% accuracy on the hardest classification task without needing to change the model architecture or train the model for a longer duration. This finding clearly demonstrates why Data Augmentation is a standard practice in Deep Learning: it effectively converts model "memorization" into robust "generalization."
\begin{figure}[h!]
    \centering
    
    % Left image
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]
        {Images/Multihead_two_heads_loss_curve.png}
        \caption*{Without Data Augmentation}
    \end{minipage}
    \hfill
    % Right image
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Multihead_two_heads_with_data_augmentation_loss_curve.png}
        \caption*{With Data Augmentation}
    \end{minipage}

\end{figure}


\subsection{Data Augmentation and Class Balancing on 2 Head (Model, Make}

	The primary goal of this stage was to enhance the model's performance specifically on the more rare classes. An analysis of the data revealed the impact of implementing class balancing: initially, the two-head model with Data Augmentation achieved 85.65\% accuracy on the Model Head and 92.71\% accuracy on the Make Head. After applying class balancing, the Model Head saw a modest increase to 86.02\%, while the Make Head accuracy slightly decreased to 91.77\%. The central observation is that while Class Balancing provided a distinct benefit to the infrequently occurring classes, it came at the cost of a minor reduction in overall consistency—an effect that can be likened to a "Robin Hood" dynamic, where resources are shifted from common to rare groups.


\subsection{Optimization Dynamics: Interference vs. Curriculum}

	\item \textbf{Objective:} Solving the "Task Interference" problem in Multi-Task Learning.
	\item \textbf{Data:} Contrast Experiment 6 (3-Head without Curriculum Learning: 85.36\%) vs. Experiment 7 (3-Head with Curriculum Learning: 86.15\%).
	\item \textbf{Key Finding:} Without Curriculum Learning, gradients conflicted. Freezing the hard head allowed the backbone to learn stable features first.


\subsection{Architectural Ablation: The Necessity of "Type"}
	The objective was to demonstrate the necessity of the architectural layers. Data comparing Experiment 8 (2-Head Curriculum: 85.51\%) against the optimized Experiment 9/10 (Final 3-Head: 87.95\%) showed a clear advantage. The key finding is that while the 3-Head approach initially struggled, once optimized, it consistently surpassed the 2-Head model. This confirms that the "Type" layer is required, serving as a necessary semantic bridge.
		

\subsection{SOTA Performance: Hierarchical Label Smoothing (HLS) \& Learning Rate Scheduler (LR Scheduler):}
	The ultimate goal was to push the model's predictive limits, leveraging the full benefit of the hierarchical structure. The data from the final optimized training run, Experiment 10, demonstrated significant results: it achieved a peak Top-1 Accuracy of 87.95\%. Crucially, the model's structural integrity improved further, with Make-Model Consistency rising from 96.60\% to 97.66\%, and Type-Model Consistency also increasing from 95.93\% to 96.58\%. This key finding shows that the combination of the Learning Rate Scheduler and Hierarchical Label Smoothing (HLS) was instrumental in driving the consistency metrics to their practical maximum.


\subsection{The Impact of Inference Strategies (Test Time Augmentation (TTA)):}

	The objective for this final stage was to achieve the ultimate accuracy improvement during the testing phase. Using the data, we show that after implementing 5-fold Test Time Augmentation (TTA), the model's accuracy reached 88.57\%, representing the highest performance we could extract from the system. This key finding confirms that a portion of the model's remaining prediction errors were attributable to view-specific noise, which the ensembling nature of TTA effectively mitigated.

\begin{table*}[t]
\centering
\caption{\textbf{Evolution of Model Performance.} Note specifically the comparison between the 2-Head Curriculum (Ablation) and 3-Head Curriculum (Phase 7), which highlights the necessity of the "Type" head to bridge the semantic gap.}
\label{tab:ablation_study}
\begin{tabular}{llccl}
\toprule
\textbf{Phase} & \textbf{Configuration} & \textbf{Acc (\%)} & \textbf{Consist. (\%)} & \textbf{Key Insight} \\
\midrule
\multicolumn{5}{l}{\textit{Baselines}} \\
1 & Baseline (Frozen ResNet) & 41.72 & 100.0* & Baseline performance. \\
2 & Fine-Tuned (Unfrozen) & 75.92 & 100.0* & Learned features, but ignored hierarchy. \\
\midrule
\multicolumn{5}{l}{\textit{Structural \& Data Improvements}} \\
3 & Multi-Head (2-Head) & 77.20 & 90.44 & Structure helps (+1.3\%), but heads conflict. \\
4 & + Data Augmentation & 85.65 & 95.26 & Robustness prevents overfitting (+8.4\%). \\
5 & + Class Balancing & 86.02 & 94.42 & Improves rare classes; slight consistency drop. \\
\midrule
\multicolumn{5}{l}{\textit{Architectural Ablation (Is the 3rd Head necessary?)}} \\
\textbf{5.5} & \textbf{2-Head + Curriculum} & \textbf{85.51} & 95.77 & \textbf{Semantic Gap:} Lacked "Body Type" bridge. \\
6 & 3-Head (No Curriculum) & 85.36 & 94.44 & \textbf{Interference:} 3 tasks confused the backbone. \\
7 & 3-Head (+ Curriculum) & 86.15 & 95.21 & \textbf{Synergy:} 3 heads + ordering $>$ 2 heads. \\
\midrule
\multicolumn{5}{l}{\textit{Final Optimization}} \\
8 & + Hierarchical Label Smoothing & 86.48 & 96.60 & Soft targets teach family structure. \\
9 & + LR Scheduler (Cosine) & 87.95 & \textbf{97.66} & Convergence into sharp minimum. \\
\textbf{10} & \textbf{+ TTA (3 Augmentations)} & \textbf{88.57} & 97.64 & \textbf{Peak Performance.} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table}[h!]
\centering

\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Top - 1 error rate} \\
\hline
Frozen Baseline & 58,28\%  \\
\hline
Unfrozen Baseline & 24,08\%  \\
\hline
Regularization and Class Balancing on 1 Head & 14,35\%  \\
\hline
Data Augmentation and Class Balancing on 2 Head & 13,98\% \\
\hline
3 Head without Curriculum Learning & 14,64\% \\
\hline
2 Head with Curriculum Learning & 14,49\% \\
\hline
3 Head with Curriculum Learning & 13,85\% \\
\hline
3 Head Hierarchical Label Smoothing and Learning Rate Scheduler & 12,05\% \\
\hline
3 Head Test Time Augmentation & 11,43\% \\
\hline
\end{tabular}
\vspace{6pt}
\caption{Top-1 error rates through experiments}

\end{table}

\subsection{Compare our results to other benchmarks:}

\begin{table}[H]
\centering
\begin{tabular}{|p{5cm}|p{4cm}|c|}
\hline
\textbf{Paper} & \textbf{Model Architecture} & \textbf{Top - 1 Accuracy} \\
\hline

A Systematic Evaluation of Recent Deep Learning Architectures for Fine-Grained Vehicle Classification &
ResNet-50 (FineTuned on ImageNet) &
92,0\% \\
\hline

& ResNet-152 (FineTuned on ImageNet) & 92,6\% \\
\hline

& DenseNet-161 (FineTuned on ImageNet) & 94,6\% \\
\hline

Fine-Grained Image Classification for Vehicle Makes \& Models using CNNs &
VGG-16 backbone (pretrained on ImageNet) &
84,0\% \\
\hline

Our Model & ResNet-50 & 88,57\% \\
\hline
\end{tabular}

\vspace{6pt}
\caption{Compare accuracy to related benchmarks}
\end{table}

\subsection{Qualitative evaluation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{Images/label_prediction_3_samples.png}
    \caption{Three Stanford-Cars test images and the five labels considered most probable by our model (3 heads, HLS, LRS). The correct label and the five labels considered most probable are written under each image. First sample: Make wrong, Type wrong, Model wrong. Second sample: Make wrong, Type wrong, Model correct. Third sample: Make correct, Type wrong, Model wrong}
    \label{fig:my_image}
\end{figure}
In Figure 3, we observe different patterns of misclassification across the three samples. \newline
The first sample demonstrates a case where the model is highly confident in its predictions for both the Make (98.86\%) and Type (92.64\%), despite both being incorrect. For the Model, the prediction is less clear, and the correct model does not even appear in the top-5 predictions. \newline
The second sample illustrates a situation where the Make and Type are misclassified, but the Model is correctly predicted. The model shows moderate confidence in predicting the wrong Make, ranking the correct one as second most likely with 19.50\%. The Type is predicted incorrectly with higher confidence (90.59\%). However, the Model is correctly identified, albeit with a relatively low confidence of 26.50\%. \newline
The final sample, featuring the Audi TTS, highlights a case where only the Make is predicted correctly. The Make ("Audi") is correctly predicted with 99.20\% confidence, while the correct Type is ranked second and the correct Model is fifth, with the Model prediction having only 4.17\% confidence. \newline
More samples for different cases of misclassifications and their top-5 predictions can be found in the appendix.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{Images/heatmap_3_samples.png}
    \caption{Three Stanford-Cars test images and the heatmaps showing on which image part our model (3 heads, HLS, LRS) focused on when producing labels for each head. The heatmaps for each head are displayed under each image. First sample: Make wrong, Type wrong, Model wrong. Second sample: Make wrong, Type wrong, Model correct. Third sample: Make correct, Type wrong, Model wrong}
    \label{fig:my_image}
\end{figure}
In Figure 4, we can observe different heatmaps for three misclassification cases.\newline
The first sample, where all heads produce incorrect labels, is taken from the side and rear perspective. It is evident that the model focuses, for each head, on the rear right side and the side of the car to generate the predictions. \newline
The second sample is captured from the front-left side, where we can see that the Make head primarily focuses on a small area above the front left tire. The Type head, in contrast, concentrates on the driver's door and the door behind it, while the Model head focuses on the bonnet, grille, and again, the area above the front left tire. \newline
In the Audi TTS example, where the image is taken from the rear, both the Make and Type heads fixate on the central region of the rear end. However, the Model head diverges significantly, focusing primarily on the left rear light and even the skyscrapers in the background. \newline
More samples for different cases of misclassifications and their heatmaps can be found in the appendix.

\section{Discussion}
In the following section, we interpret the results of our project and critically examine the factors that shaped the model’s performance. We outline which components of our approach contributed meaningfully to improvements and where limitations were faced.

\subsection{The "Frankenstein Car" Problem}
\begin{itemize}
	\item Discuss Hierarchical Consistency.
	\item In early experiments, consistency was low (~90\%). The model would predict "Toyota" (Make) and "Honda Civic" (Model).
	\item By Experiment 9 (HLS), consistency reached 96.6\%, and finally 97.66\% with the scheduler. This proves the model learned the taxonomy, not just pixel patterns.
\end{itemize}
A question in hierarchical classification is whether a model learns the underlying taxonomy or whether it simply memorizes pixel-level patterns without understanding the semantic relationships between labels. Early versions of our model frequently produced predictions where the make, type, and model belonged to different, incompatible vehicles. For example, the make head might output “Toyota”, while the model head simultaneously predicted “Honda Civic”. Such inconsistencies mean that each head is treated as an isolated classification task. \newline
In the initial experiments, hierarchical consistency remained relatively low, hovering around 90\%, meaning that in roughly one out of ten images, the predicted model did not belong to the predicted make. By introducing Hierarchical Label Smoothing (HLS) the consistency improved substantially. By Experiment 9, the model reached 96.6\% consistency which demonstrates a marked improvement in aligning predictions across heads. The final experiment, which integrated both HLS and a cosine learning rate scheduler, further increased consistency to 97.66\%. \newline
This progression shows that the model was no longer merely fitting to visual textures or superficial cues but had begun to internalize the hierarchical taxonomy of make, type, and model. The reduction of incompatible predictions illustrates that the network learned meaningful relationships between car brands and their associated models which resulted in coherent outputs.



\subsection{The Role of "Type" as Scaffolding}
\begin{itemize}
	\item Analyze why 3\_head\_curriculum eventually beat 2\_head\_curriculum.
	\item The "Type" head (Sedan, SUV, Coupe) provides Intermediate Scaffolding. It is easier to learn than "Model" but provides more structural information than "Make." It bridges the semantic gap.
\end{itemize}
A key insight from our experiments is the importance of the “Type” head (e.g., Sedan, SUV, Coupe) as an intermediate level of supervision. When comparing the two-head curriculum (Make + Model) to the three-head setup (Make + Type + Model), the latter consistently achieved higher accuracy and stability. This is because Type acts as a bridge because it is easier to learn than the fine-grained model classification but more informative than the broad make category. \newline
By giving the model an additional intermediate task, the network receives structured guidance that reduces the complexity of learning the full 196-class model label. The Type head therefore provides scaffolding that helps the model organize visual information hierarchically. This effect explains why the three-head curriculum outperformed the two-head version in both accuracy and consistency.



\subsection{The "Free Lunch" of Training Schedules}
\begin{itemize}
	\item Discuss the final experiment (LR Scheduler).
	\item ou gained ~1.5\% accuracy (87.9\% $\to$ 89.07\%) just by changing the learning rate schedule. This indicates the architecture was sound, but the optimizer needed "fine-grained" control to settle into the sharp minima of the loss landscape.
\end{itemize}
Another finding was how strongly the learning-rate schedule influenced performance. After the architecture and curriculum strategy were already working well, switching to a learning-rate scheduler brought an additional improvement of roughly 1.5 percentage points (from ~87.9\% to 89.07\% model accuracy) without modifying the model or adding new data. \newline
This highlights that the model was already capable of achieving higher performance, but the optimizer was not navigating the loss landscape efficiently. The scheduler allowed the network to make larger exploratory steps early on and gradually settle into more stable minima toward the end of training. In that sense, learning-rate scheduling was a simple change that is responsible for additional accuracy while keeping training cost and model complexity unchanged.


\subsection{Robustness via Inference Ensembling}
Explain how TTA serves as a proxy for measuring model robustness, argue that this proves our model is robust and "production-ready"

Test-time augmentation (TTA) provided an additional perspective on the robustness of the trained model. By averaging predictions over several augmented versions of each test image, such as horizontal flips or cropped resizes, the model effectively performs a lightweight ensembling step during inference. This reduces the influence of single-view biases, such as lighting, framing, or minor pose variations. \newline
In our experiments, the TTA results demonstrated that the model maintained highly stable predictions under these perturbations which confirms that it had learned generalizable visual features rather than overfitting to specific image conditions. Although the absolute performance gain from TTA was modest, the consistency across augmented views indicates that the model is resilient and can be considered “production-ready” for real-world scenarios.


\section{Conclusion}
The pursuit of fine-grained vehicle classification in this project demonstrated that while a powerful backbone like ResNet-50 is essential for feature extraction, overcoming the challenge of subtle visual differences requires more than just raw model capacity; it demands a structured learning methodology. Our initial single-head baseline, while functional, failed to capture the intrinsic make-type-model hierarchy.
\newline
The key to success lay in adopting the Shared-Bottom Multi-Task Learning architecture, specifically the three-head design. The "Type" head proved invaluable, acting as a semantic bridge that provided crucial intermediate scaffolding for the fine-grained "Model" prediction, resolving initial task interference when combined with Curriculum Learning.
\newline
Furthermore, the introduction of Hierarchical Label Smoothing (HLS) and a fine-tuned Learning Rate Scheduler were critical for squeezing out the final performance gains and enforcing logical coherence, driving prediction consistency to $97.66\%$ and virtually eliminating the "Frankenstein Car" problem.
\newline
Achieving a final Top-1 accuracy of $88.57\%$ (with TTA) validates that our strategy was sound. This project confirms that for complex, hierarchical tasks, structuring the learning process and enforcing taxonomy coherence is paramount, yielding a final model that is not only accurate but also highly resilient and logically robust against real-world variations.





\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]



\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
